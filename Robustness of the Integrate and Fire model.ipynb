{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import math\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aging_barabasi_albert_graph(N,alpha,m=2,m0=2):\n",
    "    \"\"\"\n",
    "    Implements the modified barabasi-albert model given\n",
    "    in the paper titled \"Self-organized Criticality in an \n",
    "    Integrate-and-Fire Neuron Model Based on Modified Aging Networks\".\n",
    "    \n",
    "    Each time step a random node is chosen based on its age to generate\n",
    "    a new node linked to it, which also links to its (m-1) nearest neighbors\n",
    "    with a probability given by the standard barabasi-albert model.\n",
    "    \n",
    "    N: int\n",
    "        Network size\n",
    "    alpha: float\n",
    "        Age factor which influences probabilities based on age\n",
    "    m: int\n",
    "        Number of links for each new node\n",
    "    m0: int\n",
    "        Initial size of the network\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize fully connected graph with m0 nodes\n",
    "    graph = nx.complete_graph(m0)\n",
    "    \n",
    "    age = np.ones(N)\n",
    "    for n in range(m0,N):\n",
    "        # Get degrees for all current nodes in array form\n",
    "        degrees = np.array(list(graph.degree(np.arange(n)).values()))\n",
    "        \n",
    "        # Calculate pmf for a node to generate new node\n",
    "        i_probs = degrees / age[:n]**(alpha)\n",
    "        i_probs /= np.sum(i_probs)\n",
    "        \n",
    "        # Choose a random node i\n",
    "        i = np.random.choice(np.arange(n),p=i_probs)\n",
    "        \n",
    "        # neighbors j of i\n",
    "        neighbors = np.array(graph.neighbors(i))\n",
    "        \n",
    "        # Calculate pmf for linked nearest neighbors of i\n",
    "        j_probs = degrees[neighbors] / np.sum(degrees[neighbors])\n",
    "        \n",
    "        # Choose a random node i\n",
    "        j = list(np.random.choice(neighbors,size=(m-1),replace=False,p=j_probs))\n",
    "        \n",
    "        # Add node to network with chosen edges\n",
    "        graph.add_edges_from(zip([n]*m,[i] + j))\n",
    "        \n",
    "        # Advance the time and apply to age of nodes\n",
    "        age[:n] += 1\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_if_network(num_nodes,num_edges=10,p=0.5,alpha=1,net_type=\"full\"):\n",
    "    if net_type == 'random':\n",
    "        graph = nx.gnp_random_graph(num_nodes,p)\n",
    "    elif net_type == 'ws':\n",
    "        graph = nx.watts_strogatz_graph(num_nodes, num_edges, p)\n",
    "    elif net_type == 'ba':\n",
    "        graph = nx.barabasi_albert_graph(num_nodes, num_edges)\n",
    "    elif net_type == 'aging':\n",
    "        graph = aging_barabasi_albert_graph(num_nodes,alpha,m=num_edges,m0=num_edges)\n",
    "    elif net_type == 'full':\n",
    "        graph = nx.complete_graph(num_nodes)\n",
    "        \n",
    "    return graph\n",
    "\n",
    "class simple_integrate_and_fire_model:\n",
    "    \"\"\"\n",
    "    Implements a simplified version of the integrate and fire\n",
    "    model described in https://arxiv.org/pdf/0712.1003.pdf using\n",
    "    separated time-scales where the driving rate is much slower\n",
    "    than the relaxation rate. This makes a rule-based algorithm with\n",
    "    a driving step followed by a relaxation step where an avalanche\n",
    "    is allowed to complete before the next drive step, much like\n",
    "    the BTW sandpile model.\n",
    "    \n",
    "    For the sake of simplicity the leak terms and dynamic synapses\n",
    "    are left out due to the smaller time scales at which they are influential.\n",
    "    \n",
    "    The model does include the possibility of inhibitory neurons or synapses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,network,v_ext=0.025,v_th=1,u=0.2,J=4.7,p_inh=0.0,inh_type=\"neuron\"):\n",
    "        # Set parameters\n",
    "        self.network = network\n",
    "        self.v_ext = v_ext\n",
    "        self.v_th = v_th\n",
    "        self.u = u\n",
    "        self.J = J\n",
    "        \n",
    "        # Initialize weight matrix from network\n",
    "        self.w = nx.adjacency_matrix(network)\n",
    "        \n",
    "        # Link indices in weight matrix\n",
    "        self.link_idx = (self.w != 0)\n",
    "        \n",
    "        # Network size\n",
    "        self.N = self.w.shape[0]\n",
    "        \n",
    "        # Determine synapse types\n",
    "        self.synapse_types = self.w.astype(int)\n",
    "        \n",
    "        # Inhibition determined per synapse\n",
    "        if inh_type == \"synapse\":\n",
    "            n_inh = int(p_inh * self.synapse_types.size)\n",
    "\n",
    "            type_vals = np.ones(self.synapse_types.size)\n",
    "            if n_inh > 0:\n",
    "                inhibitory = np.random.choice(np.arange(self.synapse_types.size),size=n_inh,replace=False)\n",
    "                type_vals[inhibitory] = -1\n",
    "\n",
    "            self.synapse_types[self.link_idx] = type_vals\n",
    "        \n",
    "        # Inhibition determined per neuron\n",
    "        elif inh_type == \"neuron\":\n",
    "            n_inh = int(p_inh * self.N)\n",
    "            \n",
    "            if n_inh > 0:\n",
    "                inhibitory = np.random.choice(np.arange(self.N),size=n_inh,replace=False)\n",
    "                self.synapse_types[:,inhibitory] *= -1\n",
    "        \n",
    "        # Retrieve J_ij (given as w) from w\n",
    "        self.w = J * self.w\n",
    "        \n",
    "        # Initialize membrane potentials randomly\n",
    "        self.v = np.random.uniform(0,self.v_th,self.w.shape[0])\n",
    "        \n",
    "        # Statistics\n",
    "        self.avalanche_size = np.array([],dtype=int)\n",
    "        \n",
    "    def simulate(self,steps):\n",
    "        avalanche_size = np.zeros(steps,dtype=int)\n",
    "        \n",
    "        with tqdm.tqdm(total=steps) as pbar:\n",
    "            for t in range(steps):\n",
    "                # Drive step\n",
    "                i = np.random.randint(self.v.size)\n",
    "                self.v[i] += self.v_ext\n",
    "\n",
    "                # Initialize check list\n",
    "                check_nodes = [i]\n",
    "\n",
    "                # Relaxation step\n",
    "                s = 0\n",
    "                while len(check_nodes) > 0:\n",
    "                    i = check_nodes.pop(0)\n",
    "\n",
    "                    if self.v[i] > self.v_th:\n",
    "                        # number of neighbors of i\n",
    "                        n = np.sum(self.w[:,i].size)\n",
    "\n",
    "                        # neighbor indices\n",
    "                        j = self.w[:,i].nonzero()[0]\n",
    "\n",
    "                        # Spiking results in firing potential to neighbors\n",
    "                        self.v[j] += self.synapse_types[j,i].toarray().flatten() * \\\n",
    "                                     self.u*self.w[j,i].toarray().flatten() / n\n",
    "\n",
    "                        # Add neighbors to check list\n",
    "                        check_nodes += [elem for elem in list(j) if elem not in check_nodes]\n",
    "\n",
    "                        # Subtract threshold potential after spike\n",
    "                        self.v[i] -= self.v_th\n",
    "\n",
    "                        # Increase current avalanche size\n",
    "                        s += 1\n",
    "\n",
    "                # Enforce minimum potential of zero\n",
    "                self.v[self.v < 0] = 0\n",
    "                        \n",
    "                avalanche_size[t] = s\n",
    "                \n",
    "                pbar.update()\n",
    "            \n",
    "        self.avalanche_size = np.concatenate((self.avalanche_size,avalanche_size))\n",
    "        \n",
    "    def reset_avalanche_stats(self):\n",
    "        self.avalanche_size = np.array([],dtype=int)\n",
    "        \n",
    "    def avalanche_size_pdf(self):\n",
    "        pdf = np.bincount(self.avalanche_size) / self.avalanche_size.size\n",
    "        \n",
    "        nonzeros = (pdf != 0)\n",
    "        indices = np.arange(nonzeros.size)[nonzeros]\n",
    "        \n",
    "        return indices,pdf[nonzeros]\n",
    "\n",
    "class LHG_integrate_and_fire_model:\n",
    "    \"\"\"\n",
    "    Implements the leaky integrate and fire model described in \n",
    "    https://ediss.uni-goettingen.de/bitstream/handle/11858/00-1735-0000-0006-B3B3-B/levina.pdf?sequence=1.\n",
    "    \n",
    "    The algorithm allows for leak terms, dynamic synapses and inhibition.\n",
    "    \n",
    "    The model operates on the assumption of separated time scales. Meaning that a neuronal\n",
    "    avalanche is allowed to complete long before the next external input is applied. This\n",
    "    means that the system is slowly driven and reacts quickly to any perturbation. The\n",
    "    discretization is done by setting the driving rate to 1 and setting the time step\n",
    "    size equal to the driving rate. The relaxation is assumed to be fast enough to \n",
    "    happen instantly after the driving step and to finish at around the same time (t + dt ~ t). \n",
    "    This is done due to the event driven nature of the system dynamics.\n",
    "    \n",
    "    Leakage and synapse recovery are both applied using the values at the start \n",
    "    of the time step for the time integration.\n",
    "    \n",
    "    Inhibition is implemented in two possible ways: \n",
    "    (1) Ignoring Dale's principle and taking a percentage of the \n",
    "    synapses to be inhibitory or (2) obeying Dale's principle and\n",
    "    taking a percentage of the neurons to be inhibitory.\n",
    "    \n",
    "    TODO:\n",
    "    - Implement leak terms - done (Levina claims: no relevant dynamics changes)\n",
    "    - Implement dynamic synapses - done\n",
    "    - Implement inhibitory neurons - done (Levina claims: no relevant dynamics changes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,network,v_ext=0.025,v_th=1,u=0.2,a=0.5,\n",
    "                 nu=10,tl=40,C=0.98,leakage=False,p_inh=0.0,inh_type=\"neuron\"):\n",
    "        \"\"\"\n",
    "        network: networkx network object\n",
    "            Network used for the simulation\n",
    "        v_ext: float\n",
    "            External input added to the potential \n",
    "            of a neuron during the driving step\n",
    "        v_th: float\n",
    "            Membrane potential treshold\n",
    "        u: float\n",
    "            Transmitter resource usage / \n",
    "            saturation constant of synaptic strength\n",
    "        \n",
    "        Dynamic synapse parameters:\n",
    "            a: float\n",
    "                Maximum connection strength parameter (a / u = J_max)\n",
    "            nu: float\n",
    "                Synaptic recovery time scale parameter\n",
    "        \n",
    "        Leak term parameters:\n",
    "            tl: float\n",
    "                Rate of leakage from a node\n",
    "            C: float\n",
    "                Compensatory synaptic current\n",
    "            leakage: bool\n",
    "                Determine if leakage occurs\n",
    "                \n",
    "        p_inh: float\n",
    "            Percentage of inhibitory synapses\n",
    "        inh_type: string (\"neuron\" or \"synapse\")\n",
    "            Type of inhibition: per synapse (ignoring Dale's principle)\n",
    "            or per neuron (obeying Dale's principle)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set parameters\n",
    "        self.network = network\n",
    "        self.v_ext = v_ext\n",
    "        self.v_th = v_th\n",
    "        self.u = u\n",
    "        self.a = a\n",
    "        self.nu = nu\n",
    "        self.tl = tl\n",
    "        self.C = C\n",
    "        self.leakage = leakage\n",
    "        \n",
    "        # Retrieve J_ij (weight matrix w) from network\n",
    "        self.w = nx.adjacency_matrix(network)\n",
    "        \n",
    "        # Link indices in weight matrix\n",
    "        self.link_idx = (self.w != 0)\n",
    "        \n",
    "        # Network size\n",
    "        self.N = self.w.shape[0]\n",
    "        \n",
    "        # Determine synapse types\n",
    "        self.synapse_types = self.w.astype(int)\n",
    "        \n",
    "        # Inhibition determined per synapse\n",
    "        if inh_type == \"synapse\":\n",
    "            n_inh = int(p_inh * self.synapse_types.size)\n",
    "\n",
    "            type_vals = np.ones(self.synapse_types.size)\n",
    "            if n_inh > 0:\n",
    "                inhibitory = np.random.choice(np.arange(self.synapse_types.size),size=n_inh,replace=False)\n",
    "                type_vals[inhibitory] = -1\n",
    "\n",
    "            self.synapse_types[self.link_idx] = type_vals\n",
    "        \n",
    "        # Inhibition determined per neuron\n",
    "        elif inh_type == \"neuron\":\n",
    "            n_inh = int(p_inh * self.N)\n",
    "            \n",
    "            if n_inh > 0:\n",
    "                inhibitory = np.random.choice(np.arange(self.N),size=n_inh,replace=False)\n",
    "                self.synapse_types[:,inhibitory] *= -1\n",
    "        \n",
    "        # Randomize synaptic connection strengths\n",
    "        self.w = self.w.astype(float)\n",
    "        self.w[self.link_idx] = np.random.uniform(0,self.a/self.u,self.link_idx.size)\n",
    "        \n",
    "        # Synaptic recovery time-scale\n",
    "        self.tj = self.nu * self.N\n",
    "        \n",
    "        # Initialize membrane potentials randomly\n",
    "        self.v = np.random.uniform(0,self.v_th,self.N)\n",
    "        \n",
    "        # Statistics\n",
    "        self.avalanche_size = np.array([],dtype=int)\n",
    "        \n",
    "    def simulate(self,steps):\n",
    "        # Analytics to be collected\n",
    "        avalanche_size = np.zeros(steps,dtype=int)\n",
    "        \n",
    "        weights = []\n",
    "        \n",
    "        with tqdm.tqdm(total=steps) as pbar:\n",
    "            for t in range(steps):\n",
    "                \n",
    "                weights.append(self.w)\n",
    "                \n",
    "                # Apply leakage terms\n",
    "                if self.leakage:\n",
    "                    self.v += self.C - self.v / self.tl\n",
    "                \n",
    "                # Drive step\n",
    "                i = np.random.randint(self.v.size)\n",
    "                self.v[i] += self.v_ext\n",
    "\n",
    "                # Synaptic recovery term\n",
    "                J_rec = (self.a / self.u - self.w[self.link_idx]) / self.tj\n",
    "                \n",
    "                # Initialize check list\n",
    "                check_nodes = [i]\n",
    "\n",
    "                # Relaxation step\n",
    "                s = 0\n",
    "                while len(check_nodes) > 0:\n",
    "                    i = check_nodes.pop(0)\n",
    "\n",
    "                    if self.v[i] > self.v_th:\n",
    "                        # number of neighbors of i\n",
    "                        n = np.sum(self.w[:,i].size)\n",
    "\n",
    "                        # neighbor indices\n",
    "                        j = self.w[:,i].nonzero()[0]\n",
    "\n",
    "                        # Spiking results in firing potential to neighbors\n",
    "                        # Inhibitory neurons subtract potential instead of adding\n",
    "                        self.v[j] += self.synapse_types[j,i].toarray().flatten() * \\\n",
    "                                     self.u * self.w[j,i].toarray().flatten() / n\n",
    "                        \n",
    "                        # Decrease synaptic connection strength\n",
    "                        self.w[j,i] -= self.u*self.w[j,i]\n",
    "                        \n",
    "                        # Add neighbors to check list\n",
    "                        check_nodes += [elem for elem in list(j) if elem not in check_nodes]\n",
    "\n",
    "                        # Subtract threshold potential after spike\n",
    "                        self.v[i] -= self.v_th\n",
    "\n",
    "                        # Increase current avalanche size\n",
    "                        s += 1\n",
    "\n",
    "                # Enforce minimum potential of zero\n",
    "                self.v[self.v < 0] = 0\n",
    "                \n",
    "                # Apply synaptic recovery after relaxation\n",
    "                self.w[self.link_idx] += J_rec\n",
    "                \n",
    "                avalanche_size[t] = s \n",
    "                \n",
    "                pbar.update()\n",
    "            \n",
    "            \n",
    "        self.avalanche_size = np.concatenate((self.avalanche_size,avalanche_size))\n",
    "        \n",
    "    def reset_avalanche_stats(self):\n",
    "        self.avalanche_size = np.array([],dtype=int)\n",
    "        \n",
    "    def avalanche_size_pdf(self):\n",
    "        pdf = np.bincount(self.avalanche_size) / self.avalanche_size.size\n",
    "        \n",
    "        nonzeros = (pdf != 0)\n",
    "        indices = np.arange(nonzeros.size)[nonzeros]\n",
    "        \n",
    "        return indices,pdf[nonzeros]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_rand(adj_mat, per, mode):\n",
    "## Function that deletes percentage of nodes or links randomly\n",
    "## adj_mat: adjacency matrix of network\n",
    "## per: percentage of the nodes or links that will be deleted\n",
    "## mode: determines whether nodes or links will be deleted\n",
    "## Returns: the new adjacency matrix of the network\n",
    "\n",
    "    # Copy the adjacency matrix\n",
    "    am = adj_mat.copy()\n",
    "    \n",
    "    if mode == 'node':\n",
    "        \n",
    "        # Determine exact number of nodes\n",
    "        nodes = math.ceil(adj_mat.shape[0]*per/100)\n",
    "        \n",
    "        for i in range(0,nodes-1):\n",
    "            \n",
    "            # Choose one node randomly\n",
    "            n = random.randint(0, adj_mat.shape[0]-1)\n",
    "        \n",
    "            # Delete the corresponding row and column of the matrix\n",
    "            np.delete(am, n, 0)\n",
    "            np.delete(am, n, 1)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Determine exact number of links\n",
    "        links = math.ceil(adj_mat.shape[0]*per/100)\n",
    "        \n",
    "        for i in range(0,links-1):\n",
    "            \n",
    "            # Choose one link randomly\n",
    "            l1 = random.randint(0, adj_mat.shape[0]-1)\n",
    "            l2 = random.randint(0, adj_mat.shape[0]-1)\n",
    "            \n",
    "            # Set the corresponding cell of the matrix to zero\n",
    "            am[l1,l2] = 0\n",
    "        \n",
    "    \n",
    "    return am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate & Fire model (LHG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of the Ageing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an ageing network of 50 nodes\n",
    "network = create_if_network(50,net_type=\"aging\")\n",
    "\n",
    "model = LHG_integrate_and_fire_model(network,a=1.4)\n",
    "\n",
    "model.simulate(10000)\n",
    "\n",
    "model.reset_avalanche_stats()\n",
    "\n",
    "model.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices, pdf = model.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete 2 percentages of nodes\n",
    "am20 = del_rand(nx.adjacency_matrix(model.network), 20, 'node')\n",
    "am40 = del_rand(nx.adjacency_matrix(model.network), 40, 'node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace adj matrix with the one that has 80% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am20)\n",
    "\n",
    "#Rerun the model as before\n",
    "model20 = LHG_integrate_and_fire_model(network,a=1.4)\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "#Replace adj matrix with the one that has 60% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am40)\n",
    "\n",
    "model40 = LHG_integrate_and_fire_model(network,a=1.4)\n",
    "\n",
    "model40.simulate(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices20, pdf20 = model20.avalanche_size_pdf()\n",
    "indices40, pdf40 = model40.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf, indices20, pdf20, indices40, pdf40)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend(['normal', '80%', '60%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of the Barabasi network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Barabasi network of 50 nodes\n",
    "network = create_if_network(50,net_type=\"ba\")\n",
    "\n",
    "model = LHG_integrate_and_fire_model(network,a=1.4)\n",
    "\n",
    "model.simulate(100000)\n",
    "\n",
    "model.reset_avalanche_stats()\n",
    "\n",
    "model.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices, pdf = model.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Delete 2 percentages of nodes\n",
    "am20 = del_rand(nx.adjacency_matrix(model.network), _, 20, 'node')\n",
    "am40 = del_rand(nx.adjacency_matrix(model.network), _, 40, 'node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace adj matrix with the one that has 80% of the initial nodes\n",
    "network.adjacency_matrix = am20\n",
    "\n",
    "#Rerun the model as before\n",
    "model20 = LHG_integrate_and_fire_model(network,a=0.95)\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "model20.reset_avalanche_stats()\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "#Replace adj matrix with the one that has 60% of the initial nodes\n",
    "network.adjacency_matrix = am40\n",
    "\n",
    "model40 = LHG_integrate_and_fire_model(network,a=0.95)\n",
    "\n",
    "model40.simulate(100000)\n",
    "\n",
    "model40.reset_avalanche_stats()\n",
    "\n",
    "model40.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices20, pdf20 = model20.avalanche_size_pdf()\n",
    "indices40, pdf40 = model40.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf, indices20, pdf20, indices40, pdf40)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend(['normal', '80%', '60%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of the Fully Connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a fully connected network of 50 nodes\n",
    "network = create_if_network(50,net_type=\"random\")\n",
    "\n",
    "model = LHG_integrate_and_fire_model(network,a=1.4)\n",
    "\n",
    "model.simulate(100000)\n",
    "\n",
    "model.reset_avalanche_stats()\n",
    "\n",
    "model.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices, pdf = model.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Delete 5 percentages of nodes\n",
    "am20 = del_rand(nx.adjacency_matrix(model.network), 20, 'node')\n",
    "am40 = del_rand(nx.adjacency_matrix(model.network), 40, 'node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace adj matrix with the one that has 80% of the initial nodes\n",
    "network.adjacency_matrix = am20\n",
    "\n",
    "#Rerun the model as before\n",
    "model20 = LHG_integrate_and_fire_model(network,a=0.95)\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "model20.reset_avalanche_stats()\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "#Replace adj matrix with the one that has 60% of the initial nodes\n",
    "network.adjacency_matrix = am40\n",
    "\n",
    "model40 = LHG_integrate_and_fire_model(network,a=0.95)\n",
    "\n",
    "model40.simulate(100000)\n",
    "\n",
    "model40.reset_avalanche_stats()\n",
    "\n",
    "model40.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices20, pdf20 = model20.avalanche_size_pdf()\n",
    "indices40, pdf40 = model40.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf, indices20, pdf20, indices40, pdf40)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend(['normal', '80%', '60%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate & Fire (simple) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of the Barabasi network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a barbasi network of 50 nodes\n",
    "network = create_if_network(50,net_type=\"ba\")\n",
    "\n",
    "model = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model.simulate(100000)\n",
    "\n",
    "model.reset_avalanche_stats()\n",
    "\n",
    "model.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices, pdf = model.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete 5 percentages of nodes\n",
    "am20 = del_rand(nx.adjacency_matrix(model.network), 20, 'node')\n",
    "am40 = del_rand(nx.adjacency_matrix(model.network), 40, 'node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace adj matrix with the one that has 80% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am20)\n",
    "\n",
    "#Rerun the model as before\n",
    "model20 = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "#Replace adj matrix with the one that has 60% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am40)\n",
    "\n",
    "model40 = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model40.simulate(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices20, pdf20 = model20.avalanche_size_pdf()\n",
    "indices40, pdf40 = model40.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf, indices20, pdf20, indices40, pdf40)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend(['normal', '80%', '60%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of the Ageing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an ageing network of 50 nodes\n",
    "network = create_if_network(50,net_type=\"aging\")\n",
    "\n",
    "model = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model.simulate(100000)\n",
    "\n",
    "model.reset_avalanche_stats()\n",
    "\n",
    "model.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices, pdf = model.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete 5 percentages of nodes\n",
    "am20 = del_rand(nx.adjacency_matrix(model.network), 20, 'node')\n",
    "am40 = del_rand(nx.adjacency_matrix(model.network), 40, 'node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace adj matrix with the one that has 80% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am20)\n",
    "\n",
    "#Rerun the model as before\n",
    "model20 = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "#Replace adj matrix with the one that has 60% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am40)\n",
    "\n",
    "model40 = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model40.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices20, pdf20 = model20.avalanche_size_pdf()\n",
    "indices40, pdf40 = model40.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf, indices20, pdf20, indices40, pdf40)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend(['normal', '80%', '60%'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case of the Fully Connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a fully connected network of 50 nodes\n",
    "network = create_if_network(50,net_type=\"full\")\n",
    "\n",
    "model = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model.simulate(100000)\n",
    "\n",
    "model.reset_avalanche_stats()\n",
    "\n",
    "model.simulate(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices, pdf = model.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete 2 percentages of nodes\n",
    "am20 = del_rand(nx.adjacency_matrix(model.network), 20, 'node')\n",
    "am40 = del_rand(nx.adjacency_matrix(model.network), 40, 'node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace adj matrix with the one that has 80% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am20)\n",
    "\n",
    "#Rerun the model as before\n",
    "model20 = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model20.simulate(100000)\n",
    "\n",
    "#Replace adj matrix with the one that has 60% of the initial nodes\n",
    "network = nx.convert_matrix.from_scipy_sparse_matrix(am40)\n",
    "\n",
    "model40 = simple_integrate_and_fire_model(network)\n",
    "\n",
    "model40.simulate(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Average size pdf\")\n",
    "plt.ylabel(\"P(s)\")\n",
    "plt.xlabel(\"s\")\n",
    "\n",
    "indices20, pdf20 = model20.avalanche_size_pdf()\n",
    "indices40, pdf40 = model40.avalanche_size_pdf()\n",
    "\n",
    "plt.plot(indices, pdf, indices20, pdf20, indices40, pdf40)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend(['normal', '80%', '60%'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
